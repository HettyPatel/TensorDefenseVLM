# Configuration for tensor decomposition defense experiments with LLaVA

# Random seed for reproducibility
random_seed: 42

# Results directory (will be appended with timestamp)
results_dir: "results_llava"

# Maximum number of batches to process (set to null to process all)
max_batches: 5

# Model configuration
model:
  type: "llava" # "clip" or "llava"
  name: "llava-hf/llava-1.5-7b-hf" # Model identifier

# Dataset configuration
dataset:
  name: "laion/coco" # Dataset identifier
  split: "validation" # Dataset split
  max_samples: 500 # Maximum number of samples to use (smaller for LLaVA due to higher resource usage)
  batch_size: 8 # Batch size for dataloader (smaller for LLaVA due to higher memory usage)
  num_workers: 4 # Number of worker processes for dataloader

# Attack configuration
attack:
  type: "pgd" # "pgd" or "fgsm"
  epsilon: 0.0313 # 8/255
  steps: 2 # Number of attack steps (for PGD)
  step_size: 0.0235 # 6/255 (for PGD)
  random_start: true # Whether to use random initialization (for PGD)

# Defense configurations
defenses:
  # CP Decomposition on final normalization layer
  - name: "CP Final Norm (R=64, alpha=0.5)"
    type: "targeted"
    method: "cp"
    rank: 64
    alpha: 0.5
    target_layer: "final_norm"
    vision_layer_idx: -1

  # Compare different alpha values
  - name: "CP Final Norm (R=64, alpha=0.3)"
    type: "targeted"
    method: "cp"
    rank: 64
    alpha: 0.3
    target_layer: "final_norm"
    vision_layer_idx: -1

  # Compare different decomposition methods
  - name: "Tucker Final Norm (R=64, alpha=0.5)"
    type: "targeted"
    method: "tucker"
    rank: 64
    alpha: 0.5
    target_layer: "final_norm"
    vision_layer_idx: -1

  # Compare different layer types
  - name: "CP Attention (R=64, alpha=0.5)"
    type: "targeted"
    method: "cp"
    rank: 64
    alpha: 0.5
    target_layer: "attention"
    vision_layer_idx: -1
